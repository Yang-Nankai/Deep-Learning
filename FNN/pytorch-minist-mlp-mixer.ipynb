{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MLP-Mixer进行手写体识别\n",
    "对于MNIST手写体识别任务，可以使用MLP-Mixer模型来构建一个分类器。具体来说，可以将MNIST图像转换为向量形式，并将其输入到MLP-Mixer模型中进行分类。在训练过程中，可以使用交叉熵损失函数来衡量模型在MNIST数据集上的分类性能，并使用优化算法对模型参数进行更新。在测试过程中，可以使用训练好的模型对新的手写数字图像进行分类。\n",
    "[参考学习](https://arxiv.org/pdf/2105.01601.pdf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using PyTorch version: 2.0.0  Device: cpu\n"
     ]
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torchvision import datasets, transforms\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device('cuda')\n",
    "else:\n",
    "    device = torch.device('cpu')\n",
    "    \n",
    "print('Using PyTorch version:', torch.__version__, ' Device:', device)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data\n",
    "\n",
    "Next we'll load the MNIST data.  First time we may have to download the data, which can take a while.\n",
    "\n",
    "Note that we are here using the MNIST test data for *validation*, instead of for testing the final model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 32 #批处理大小\n",
    "\n",
    "#下载数据集\n",
    "train_dataset = datasets.MNIST('./data', \n",
    "                               train=True, \n",
    "                               download=True, \n",
    "                               transform=transforms.ToTensor())\n",
    "\n",
    "validation_dataset = datasets.MNIST('./data', \n",
    "                                    train=False, \n",
    "                                    transform=transforms.ToTensor())\n",
    "#加载数据集\n",
    "train_loader = torch.utils.data.DataLoader(dataset=train_dataset, \n",
    "                                           batch_size=batch_size, \n",
    "                                           shuffle=True)\n",
    "\n",
    "validation_loader = torch.utils.data.DataLoader(dataset=validation_dataset, \n",
    "                                                batch_size=batch_size, \n",
    "                                                shuffle=False)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "MLP-Mixer的实现\n",
    "这里参考的[mlp-mixer-pytorch](https://github.com/lucidrains/mlp-mixer-pytorch)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import nn\n",
    "from functools import partial\n",
    "from einops.layers.torch import Rearrange, Reduce\n",
    "\n",
    "pair = lambda x: x if isinstance(x, tuple) else (x, x)\n",
    "\n",
    "class PreNormResidual(nn.Module):\n",
    "    def __init__(self, dim, fn):\n",
    "        super().__init__()\n",
    "        self.fn = fn\n",
    "        self.norm = nn.LayerNorm(dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.fn(self.norm(x)) + x\n",
    "\n",
    "def FeedForward(dim, expansion_factor = 4, dropout = 0., dense = nn.Linear):\n",
    "    inner_dim = int(dim * expansion_factor)\n",
    "    return nn.Sequential(\n",
    "        dense(dim, inner_dim),\n",
    "        nn.GELU(),\n",
    "        nn.Dropout(dropout),\n",
    "        dense(inner_dim, dim),\n",
    "        nn.Dropout(dropout)\n",
    "    )\n",
    "\n",
    "def MLPMixer(*, image_size, channels, patch_size, dim, depth, num_classes, expansion_factor = 4, expansion_factor_token = 0.5, dropout = 0.):\n",
    "    image_h, image_w = pair(image_size)\n",
    "    assert (image_h % patch_size) == 0 and (image_w % patch_size) == 0, 'image must be divisible by patch size'\n",
    "    num_patches = (image_h // patch_size) * (image_w // patch_size)\n",
    "    chan_first, chan_last = partial(nn.Conv1d, kernel_size = 1), nn.Linear\n",
    "\n",
    "    return nn.Sequential(\n",
    "        Rearrange('b c (h p1) (w p2) -> b (h w) (p1 p2 c)', p1 = patch_size, p2 = patch_size),\n",
    "        nn.Linear((patch_size ** 2) * channels, dim),\n",
    "        *[nn.Sequential(\n",
    "            PreNormResidual(dim, FeedForward(num_patches, expansion_factor, dropout, chan_first)),\n",
    "            PreNormResidual(dim, FeedForward(dim, expansion_factor_token, dropout, chan_last))\n",
    "        ) for _ in range(depth)],\n",
    "        nn.LayerNorm(dim),\n",
    "        Reduce('b n c -> b c', 'mean'),\n",
    "        nn.Linear(dim, num_classes)\n",
    "    )"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "定义相关超参数并使用超参数初始化模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torchvision import datasets, transforms\n",
    "import torchvision\n",
    "from torch.autograd import Variable\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "n_epochs = 10\n",
    "learning_rate = 0.01\n",
    "momentum = 0.5\n",
    "log_interval = 10\n",
    "random_seed = 1\n",
    "torch.manual_seed(random_seed)\n",
    "img_height = 28\n",
    "img_width = 28\n",
    "\n",
    "DEVICE = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "model = MLPMixer(\n",
    "    image_size = 28,\n",
    "    patch_size = 7,\n",
    "    dim = 14,\n",
    "    depth = 3,\n",
    "    num_classes = 10,\n",
    "    channels  = 1\n",
    ")\n",
    "\n",
    "model.to(DEVICE)\n",
    "optimizer = torch.optim.Adam(model.parameters(),lr=learning_rate)\n",
    "mse = nn.MSELoss()\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "定义训练函数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 定义训练函数\n",
    "def train(model, device, train_loader, optimizer, epoch):\n",
    "    model.train()\n",
    "    for batch_idx, (data, target) in enumerate(train_loader):\n",
    "        batch_size_train = data.shape[0]\n",
    "        data, target = data.to(device), target.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        pre_out = model(data)\n",
    "        targ_out = torch.nn.functional.one_hot(target,num_classes=10)\n",
    "        targ_out = targ_out.view((batch_size_train,10)).float()\n",
    "        loss = mse(pre_out, targ_out)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        if (batch_idx + 1) % 300 == 0:\n",
    "            print('Train Epoch: {} [{}/{} ({:.0f}%)]tLoss: {:.6f}'.format(\n",
    "                epoch, batch_idx * len(data), len(train_loader.dataset),\n",
    "                100. * batch_idx / len(train_loader), loss.item()))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "定义测试函数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 定义测试函数\n",
    "def validate(model, device, test_loader):\n",
    "    model.eval()\n",
    "    test_loss, correct =0,0\n",
    "    with torch.no_grad():\n",
    "        for data, target in test_loader:\n",
    "            batch_size_test = data.shape[0]\n",
    "            data, target = data.to(device), target.to(device)\n",
    "            pre_out = model(data)\n",
    "            targ_out = torch.nn.functional.one_hot(target,num_classes=10)\n",
    "            targ_out = targ_out.view((batch_size_test,10)).float()\n",
    "            test_loss += mse(pre_out, targ_out) # 将一批的损失相加\n",
    "            pred = pre_out.data.max(1)[1] # get the index of the max log-probability\n",
    "            correct += pred.eq(target.data).cpu().sum()\n",
    "    \n",
    "    test_loss /= len(test_loader.dataset)\n",
    "\n",
    "    accuracy = 100. * correct.to(torch.float32) / len(validation_loader.dataset)\n",
    "    \n",
    "    print('\\nValidation set: Average loss: {:.4f}, Accuracy: {}/{} ({:.0f}%)\\n'.format(\n",
    "        test_loss, correct, len(validation_loader.dataset), accuracy))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "进行测试迭代，并保存相关模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 0 [9568/60000 (16%)]tLoss: 0.006092\n",
      "Train Epoch: 0 [19168/60000 (32%)]tLoss: 0.005909\n",
      "Train Epoch: 0 [28768/60000 (48%)]tLoss: 0.008654\n",
      "Train Epoch: 0 [38368/60000 (64%)]tLoss: 0.000706\n",
      "Train Epoch: 0 [47968/60000 (80%)]tLoss: 0.007387\n",
      "Train Epoch: 0 [57568/60000 (96%)]tLoss: 0.010439\n",
      "\n",
      "Validation set: Average loss: 0.0002, Accuracy: 9617/10000 (96%)\n",
      "\n",
      "Train Epoch: 1 [9568/60000 (16%)]tLoss: 0.005884\n",
      "Train Epoch: 1 [19168/60000 (32%)]tLoss: 0.002325\n",
      "Train Epoch: 1 [28768/60000 (48%)]tLoss: 0.004913\n",
      "Train Epoch: 1 [38368/60000 (64%)]tLoss: 0.009388\n",
      "Train Epoch: 1 [47968/60000 (80%)]tLoss: 0.002753\n",
      "Train Epoch: 1 [57568/60000 (96%)]tLoss: 0.001184\n",
      "\n",
      "Validation set: Average loss: 0.0002, Accuracy: 9659/10000 (97%)\n",
      "\n",
      "Train Epoch: 2 [9568/60000 (16%)]tLoss: 0.006323\n",
      "Train Epoch: 2 [19168/60000 (32%)]tLoss: 0.010203\n",
      "Train Epoch: 2 [28768/60000 (48%)]tLoss: 0.008023\n",
      "Train Epoch: 2 [38368/60000 (64%)]tLoss: 0.009948\n",
      "Train Epoch: 2 [47968/60000 (80%)]tLoss: 0.008716\n",
      "Train Epoch: 2 [57568/60000 (96%)]tLoss: 0.004011\n",
      "\n",
      "Validation set: Average loss: 0.0002, Accuracy: 9599/10000 (96%)\n",
      "\n",
      "Train Epoch: 3 [9568/60000 (16%)]tLoss: 0.004195\n",
      "Train Epoch: 3 [19168/60000 (32%)]tLoss: 0.007010\n",
      "Train Epoch: 3 [28768/60000 (48%)]tLoss: 0.005078\n",
      "Train Epoch: 3 [38368/60000 (64%)]tLoss: 0.000828\n",
      "Train Epoch: 3 [47968/60000 (80%)]tLoss: 0.002172\n",
      "Train Epoch: 3 [57568/60000 (96%)]tLoss: 0.009119\n",
      "\n",
      "Validation set: Average loss: 0.0002, Accuracy: 9625/10000 (96%)\n",
      "\n",
      "Train Epoch: 4 [9568/60000 (16%)]tLoss: 0.005708\n",
      "Train Epoch: 4 [19168/60000 (32%)]tLoss: 0.007316\n",
      "Train Epoch: 4 [28768/60000 (48%)]tLoss: 0.005900\n",
      "Train Epoch: 4 [38368/60000 (64%)]tLoss: 0.000410\n",
      "Train Epoch: 4 [47968/60000 (80%)]tLoss: 0.008370\n",
      "Train Epoch: 4 [57568/60000 (96%)]tLoss: 0.001393\n",
      "\n",
      "Validation set: Average loss: 0.0002, Accuracy: 9641/10000 (96%)\n",
      "\n",
      "Train Epoch: 5 [9568/60000 (16%)]tLoss: 0.000690\n",
      "Train Epoch: 5 [19168/60000 (32%)]tLoss: 0.012770\n",
      "Train Epoch: 5 [28768/60000 (48%)]tLoss: 0.012299\n",
      "Train Epoch: 5 [38368/60000 (64%)]tLoss: 0.005935\n",
      "Train Epoch: 5 [47968/60000 (80%)]tLoss: 0.000512\n",
      "Train Epoch: 5 [57568/60000 (96%)]tLoss: 0.006698\n",
      "\n",
      "Validation set: Average loss: 0.0002, Accuracy: 9688/10000 (97%)\n",
      "\n",
      "Train Epoch: 6 [9568/60000 (16%)]tLoss: 0.010024\n",
      "Train Epoch: 6 [19168/60000 (32%)]tLoss: 0.003642\n",
      "Train Epoch: 6 [28768/60000 (48%)]tLoss: 0.001388\n",
      "Train Epoch: 6 [38368/60000 (64%)]tLoss: 0.000791\n",
      "Train Epoch: 6 [47968/60000 (80%)]tLoss: 0.002638\n",
      "Train Epoch: 6 [57568/60000 (96%)]tLoss: 0.004291\n",
      "\n",
      "Validation set: Average loss: 0.0002, Accuracy: 9662/10000 (97%)\n",
      "\n",
      "Train Epoch: 7 [9568/60000 (16%)]tLoss: 0.001965\n",
      "Train Epoch: 7 [19168/60000 (32%)]tLoss: 0.006029\n",
      "Train Epoch: 7 [28768/60000 (48%)]tLoss: 0.006110\n",
      "Train Epoch: 7 [38368/60000 (64%)]tLoss: 0.007754\n",
      "Train Epoch: 7 [47968/60000 (80%)]tLoss: 0.001404\n",
      "Train Epoch: 7 [57568/60000 (96%)]tLoss: 0.002722\n",
      "\n",
      "Validation set: Average loss: 0.0002, Accuracy: 9656/10000 (97%)\n",
      "\n",
      "Train Epoch: 8 [9568/60000 (16%)]tLoss: 0.011865\n",
      "Train Epoch: 8 [19168/60000 (32%)]tLoss: 0.000701\n",
      "Train Epoch: 8 [28768/60000 (48%)]tLoss: 0.003098\n",
      "Train Epoch: 8 [38368/60000 (64%)]tLoss: 0.008299\n",
      "Train Epoch: 8 [47968/60000 (80%)]tLoss: 0.005392\n",
      "Train Epoch: 8 [57568/60000 (96%)]tLoss: 0.000924\n",
      "\n",
      "Validation set: Average loss: 0.0002, Accuracy: 9667/10000 (97%)\n",
      "\n",
      "Train Epoch: 9 [9568/60000 (16%)]tLoss: 0.010931\n",
      "Train Epoch: 9 [19168/60000 (32%)]tLoss: 0.015323\n",
      "Train Epoch: 9 [28768/60000 (48%)]tLoss: 0.006195\n",
      "Train Epoch: 9 [38368/60000 (64%)]tLoss: 0.007345\n",
      "Train Epoch: 9 [47968/60000 (80%)]tLoss: 0.004872\n",
      "Train Epoch: 9 [57568/60000 (96%)]tLoss: 0.002345\n",
      "\n",
      "Validation set: Average loss: 0.0002, Accuracy: 9668/10000 (97%)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(n_epochs):               \n",
    "    train(model, DEVICE, train_loader, optimizer, epoch)\n",
    "    validate(model, DEVICE, validation_loader)\n",
    "    torch.save(model.state_dict(), './model.pth')\n",
    "    torch.save(optimizer.state_dict(), './optimizer.pth')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
